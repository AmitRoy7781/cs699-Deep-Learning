\section*{Programming Part}

In this part, you are going to implement multiple (1) gradient descent variants
and (2) regularization methods (3) HMC sampler for GMM parameters.
The rule of thumbs is that you can do any changes, but files which are not
marked (by red color in \ref{subsubsec:overview}, e.g.,
``main.py'') will be overwritten by TA in the test stage.

\hfill

\textbf{A GPU is essential for some of the tasks, thus make sure you follow the instruction to set up the GPU environment on ``scholar.rcac.purdue.edu''.} A tutorial is available at\\
{\scriptsize \url{https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2023/howto/cluster-how-to.html}}.


We provide a brief command summary for GPU environment setup on ``scholar-fe04.rcac.purdue.edu'' to ``scholar-fe06.rcac.purdue.edu''.
Pay attention that you should use {\bf Python 3.8, CUDA 10.1 and Pytorch 1.8.0} for best support.
\begin{lstlisting}[language=bash]
module load learning/conda-5.1.0-py36-gpu
conda create -n DPLClass python=3.8 ipython ipykernel
source activate DPLClass
module load cuda/11.8.0
module load cudnn
conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia
conda install torchvision -c pytorch
conda install seaborn
conda install scikit-learn
\end{lstlisting}

Note that every time you login to the scholar cluster, you should run the $1,3,4,5$ commands again.

\subsection*{HW Overview}
\label{subsubsec:overview}
There are three tasks in this homework. The first two tasks, namely optimizers and regularizations task, will use the codebase named ``hw2\_optimizer\_skeleton''.

\section{Q2: Optimizers and regularizations}
You are going to create in a few new components to the HW2 package located at \\ \url{https://www.dropbox.com/scl/fo/d2fotkxx256vys64rc5rf/h?dl=0&rlkey=e2uya49wtvw7ii1agwpluzesc}.

\noindent Here is the folder structure that you should use:

 \hfill

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{core}=[draw=blue,fill=blue!30]
\tikzstyle{optional}=[dashed,draw=red,fill=gray!50]
\begin{tikzpicture}[%
  grow via three points={one child at (0.5,-0.7) and
  two children at (0.5,-0.7) and (0.5,-1.4)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \node {hw\homeworknumber\_optimizer\_skeleton}
    child { node [selected] {ReadMe}}
    child { node {download.py}}
    child { node [selected] {models.py}}
    child { node [selected] {optimizers.py}}
    child { node [core] {main.py}}
    child { node {interface.sh}}
    child { node {visualize.py}};
\end{tikzpicture}

\hfill

\begin{itemize}
\item \textbf{hw2\_optimizer\_skeleton}: the top-level folder that contains all the files
          required in this homework.


\item \textbf{ReadMe}: Your ReadMe should begin with a couple of \textbf{example commands}, e.g., "python hw\homeworknumber.py data", used to generate the outputs you report. TA would replicate your results with the commands
          provided here. More detailed options, usages and designs of your
          program can be followed. You can also list any concerns that you
          think TA should know while running your program. Note that put the
          information that you think it's more important at the top. Moreover,
          the file should be written in pure text format that can be displayed
          with Linux "less" command.
          You can refer to interface.sh for an example.

\item \textbf{main.py}: The \underline{main executable} to run training with different minibatch SGD algorithms.
\item \textbf{download.py}: The \underline{executable script} to download all essential data for this homework.
\item \textbf{visualize.py}: The \underline{executable script} to render plots for your results.
          It can give you a better understanding of your implementations.

\item \textbf{interface.sh}: The executable bash script to give you examples of main.py usage. It also works as an example for writing ReadMe.

\item \textbf{models.py}: The module defines the learning framework for this homework.
    You will need to {\bf \underline{fill this file}} as homework.

\item \textbf{optimizers.py}: The module defines the customized optimizers for this homework.
        You will need to {\bf \underline{fill this file}} as homework.


The two modules that you are going to develop:
\begin{itemize}
\item \textbf{models.py}
\item \textbf{optimizers.py}
\end{itemize}
The detail will be provided in the task descriptions. All other modules are just there for your convenience. It is not requried to use them, but exploring that will be a good practice of re-using code. Again, you are welcome to architect the package in your own favorite. For instance, adding another module, called \texttt{utils.py}, to facilitate your implementation.

\end{itemize}


\subsection*{Data: MNIST}

You are going to conduct a simple classification task, called MNIST (\url{http://yann.lecun.com/exdb/mnist/}). It classifies images of hand-written digits (0-9). Each example thus is a \(28 \times 28\) image. 
\begin{itemize}
\item The full dataset contains 60k training examples and 10k testing examples.
\item We provide \textbf{download.py} that will automatically download the data. Make sure that torchvision library is available.
\end{itemize}



\subsection{Gradient Descent Variants}

Gradient Descent does its job decently when the dataset is small. However, that is not the case we see in Deep Learning. Multiple variants of learning algorithms have been proposed to realize training with huge amount of data. We've learned several in the class. Now it's time to test your understanding about them.

\hfill

\noindent \textbf{Task 1a: Minibatch Stochastic Gradient Descent (SGD) {\bf (1 pt)}} 

\hfill

\noindent This is a warm-up task. You are going adapt your HW1 from full-batch to minibatch SGD.
All the codes are given.


\hfill

\noindent In Minibatch SGD, the gradients come from minibatches:
\begin{align}
    L(W) &= \frac{1}{N_b} \sum_{i=1}^{N_b} L_i(x_i, y_i, W) \\
    \nabla_W L(W) &= \frac{1}{N_b} \sum_{i=1}^{N_b} \nabla_W L_i(x_i, y_i, W),
\end{align}
where $N_b$ is the number of examples in one batch. It is similar to the full-batch case, but now you only feed a subset of the training data and update the weights according to the gradients calculated from this subset.

\hfill

\noindent Related Modules: 
\begin{itemize} 
\item main.py
\item models.py
\item optimizers.py
\end{itemize}

\hfill

\noindent Action Items:
\begin{enumerate}

\item Go through all the related modules. Specifically, you should understand the \textbf{main.main(.)} and \textbf{main.train(.)} well.
  Note that the ``main.train'' method will use minibatch, but does the same thing as full batch in HW1: consider all the input examples and update the weights.

\item You should notice that we move the weight updates to the \textbf{optimizers.py}. The \textbf{optimizers.SGD} is fully implemented. You should look into it and understand its interactions with the network. Inside {\bf optimizers.py} you will find a basic skeleton for your implementation. You are required to fill in the missing parts of the code, denoted with ``\# YOU SHOULD FILL IN THIS FUNCTION''.


\item Now, move to \textbf{main.py}. This file takes care of the main training framework for all experiments.
    \begin{enumerate}
        \item There is a \verb|--batch-size| command-line option that specifies that minibatch size. And you should use the ``main.train(.)'' for training with minibatches.

        \item Run your code with the command \verb|python main.py --source data/mnist --batch-size 300| (your data folder will be called "data"), which specifies that minibatch size is 300 and in default the maximum number of epochs is 50 (for faster results you can use a smaller epoch). 
    \end{enumerate}

\item Report your results by filling up Table \ref{table:minibatch} for \textbf{losses, training accuracies, and testing accuracies} (You should have 3 tables, and training is not validation). Use the batch sizes and learning rates given and report the results in the final epoch (they are reported in the log at the end of execution in the ``*.stdout.txt'' in ``sbatch'' folder). You can choose a fixed number of epoch for all the results (e.g., 20 epochs). Describe your observations in the report. A short example to run each of the experiment is \verb|python main.py --sbatch account:scholar --batch-size minibatch --lr lr| where minibatch and lr are the minibatch size and learning rate, respectively. Complete examples are also provided at \texttt{interface.sh}. ({\bf PDF report on Gradescope})

\item Running \verb|python visualize.py --minibatch| can output two plots: "Loss vs. Epochs" (add \verb|--ce| argument) and "Accuracies vs. Epochs," if you collect the results correctly in the code. Also, ``main.py'' will save running logs ``ptlog/*.stdout.txt'' to ``sbatch'' folder. Make use of those plots and logs for debugging and analyzing the models. You do not need to submit them in the report.

\item Running the program without specifying the batch size (remove the \verb|--batch-size 300| option or replace by \verb|--batch-size -1| option) gives you the regular Gradient Descent (GD). Compare the results of using regular GD and minibatch SGD, and state your observations in the {\bf PDF report on Gradescope}.  

\end{enumerate}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\diaghead(-2,1){aaaaaaaaaaaaaaa}%
{BatchSize}{LearningRate} & 1e-3 & 1e-4 & 1e-5 \\ \hline
100                    &      &      &      \\ \hline
500                    &      &      &      \\ \hline
3000                   &      &      &      \\ \hline
5000                   &      &      &      \\ \hline
\end{tabular}
\caption{The results should be reported.}
\label{table:minibatch}
\end{table}

\hfill 
\newpage
\noindent \textbf{Task 1b: Adaptive Learning Rate Algorithms {\bf (2 pts)}}

\hfill 

\noindent You should have learned the adaptive learning rate algorithms in the lecture. In addition to SGD, here we are going to implement several more: \textbf{Momentum, Nesterov Momentum, and Adam}.


\hfill 

\noindent Since each algorithm might have minor versions, here we define the exact one we want you to implement:
\begin{itemize}
\item \textbf{SGD}: (You already have it. Just for reference). For each parameter $x_t$ at the iteration $t$, you update it with:
\begin{align}
x_{t+1} &= x_t - \alpha \nabla f(x_t),
\end{align}
where $\alpha$ is the learning rate.

\item \textbf{Momentum}: for each parameter $x_t$ at the iteration $t$, and the corresponding velocity $v_t$, we have
\begin{align}
v_{t+1} &= \rho v_t + \alpha \nabla f(x_t) \\
x_{t+1} &= x_t -  v_{t+1},
\end{align}
where $\alpha$ is the learning rate, $\rho$ is the hyperparameter for the momentum rate. A common default option for $\rho$ is $0.9$.

\item \textbf{Nesterov Momentum}: for each parameter $x_t$ at the iteration $t$, and the corresponding velocity $v_t$, we have
\begin{align}
v_{t+1} &= \rho v_t + \alpha  \nabla f(x_t - \rho v_t) \\
x_{t+1} &= x_t - v_{t+1},
\end{align}
where $\alpha$ is the learning rate, $\rho$ is the hyperparameter for the momentum rate. A common default option for $\rho$ is $0.9$.

\item \textbf{Adam}: for each parameter $x_t$ at the iteration $t$, we have
\begin{align}
m_1 &= \beta_1 * m_1 + (1-\beta_1) \nabla f(x_t) \\
m_2 &= \beta_2 * m_2 + (1-\beta_2) (\nabla f(x_t))^2 \\
u_1 &= \frac{m_1}{1 - \beta_1^t} \\
u_2 &= \frac{m_2}{1 - \beta_2^t} \\
x_{t+1} &= x_t - \alpha \frac{u_1}{(\sqrt{u_2} + \epsilon)} ,
\end{align}
where $\alpha$ is the learning rate, $m_1$ and $m_2$ are the first and second moments, $u_1$ and $u_2$ are the first and second moments' bias  correction, and $\beta_1$, $\beta_2$, and $\epsilon$ are hypterparameters. A set of common choices of the parameters are $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$. We initialize $m_1 = m_2 = 0$.

\end{itemize}

\hfill



\noindent Related Modules: 
\begin{itemize}
\item optimizers.py
\end{itemize}

\noindent  Action Items (points uniformly distributed among SGD, Momentum, Nesterov and Adam implementations):
\begin{enumerate}

\item The corresponding class prototypes for the later three algorithms are given in the skeleton. You should finish each of them.
Pay attention that Nesterov additionally requires implementation for ``prev(.)'' method which is different from other optimizers.
({\bf code, not gradescope})

\item There is a \verb|--optim-alg| command-line option in main.py. So you can test your optimizers individually with the following commands:
    \begin{itemize}
    \item \verb|python main.py --optim-alg sgd --batch-size 100|
    \item \verb|python main.py --optim-alg momentum --batch-size 100|
    \item \verb|python main.py --optim-alg nesterov --batch-size 100|
    \item \verb|python main.py --optim-alg adam --batch-size 100|
    \end{itemize}
Full examples are also provided in \texttt{interface.sh}.

\item Run \verb|python visualize.py --optimizer|. Plot "Loss vs. Epochs" (Add \verb|--ce| argument) and "Accuracies vs. Epochs" for each algorithm (include SGD) and attach them to the {\bf PDF report on Gradescope}. You should have 2 plots in total containing all four optimizers.

\item Describe your observations from the plots in the {\bf PDF report on Gradescope}. Make algorithm comparisons.

\end{enumerate}

\subsection{Regularization}

\noindent You will be implementing two common regularization methods in neural networks and analyze their difference.


\hfill

\noindent \textbf{Task 2a: L2-Regularization {\bf (1 pt)} }

\hfill 

\noindent The first method is L2-Regularization. It is a general method that not only works for neural networks but also for many other parameterized learning models. It has a hyperparameter for determining the weight of the L2 term, i.e., it is the coefficient of the L2 term. Let's call it $\lambda$. In this task, you should implement the L2-regularization and conduct hyperparameter tuning for $\lambda$. Think about what should be added to the gradients when you have a L2-Regularization term in the objective function. 

\hfill

\noindent Here we give the mathematical definition of the L2-regularized objective function that you should look at:
\begin{align}
L &= \frac{1}{N} \sum_{i=1}^N L_i(x_i, y_i, W) + \lambda R(W) \label{eq:l2}\\
R(W) &= \sum_k \sum_j W_{k, j}^2,
\end{align}
where $\lambda$ is a hyperparameter to determine the weight of the regularization part. You should think about how to change the gradients according to the added regularization term.

\hfill

\noindent Related Modules: 
\begin{itemize}
\item optimizers.py
\end{itemize}

\noindent Action Items:
\begin{enumerate}
\item There is a command-line option \verb|--l2-lambda|, which specifies the coefficient of the L2 term. In defaut, it is zero, which means no regularization.

\item Add L2-Regularization to all of your optimizers with respect to the command-line option. You need to make changes for all the methods to support L2-regularization, especially for the ``SGD.step(.)''.

\item Test your network with the following commands:
    \begin{itemize}
    \item \verb|python main.py --l2-lambda 1 --batch-size 100|
    \item \verb|python main.py --l2-lambda 0.1 --batch-size 100|
    \item \verb|python main.py --l2-lambda 0.01 --batch-size 100|
    \end{itemize}
Complete examples are also provided at \texttt{interface.sh}.

\item Run \verb|python visualize.py --regularization|. Plot "Loss vs. Epochs" (Add \verb|--ce| argument) and "Accuracies vs. Epochs" for each lambda value, and include them in the {\bf PDF report on Gradescope}. You should have 2 plots in total containing all three settings.

\item Describe your observations from the plots in the PDF report on Gradescope. Make comparisons.

\end{enumerate}




