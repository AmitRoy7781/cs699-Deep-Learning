\documentclass{article}

%\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{url}

%\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{listings}

\newcommand{\class}{ CS69000-DPL Spring 2023 }
\newcommand{\website}{{\tiny\url{https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2023}}}
\newcommand{\homeworknumber}{1}
\newcommand{\duedate}{ {\bf 6:00pm}, Friday, February 17th (open until 6:00am next day)}
\newcommand{\code}{}

%\pagestyle{headings}
\setlength{\parskip}{1pc}
\setlength{\parindent}{0pt}
\setlength{\topmargin}{-1pc}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\newcommand{\var}{\mbox{var}}
\newcommand{\cov}{\mbox{cov}}

\newcommand{\newpart}{
\stepcounter{partno}
\noindent
{\bf (\alph{partno})}
}

\newcommand{\header}{
\newpage
\noindent
\framebox{ \vbox{\class Homework \hfill --- Homework \homeworknumber --- \hfill  Last update: \today
  \\ \website \hfill {\color{red} Due: \duedate} }}
\bigskip
\newline
{\bf Instructions and Policy:} Each student should write up their own solutions independently, no copying of any form is allowed.
You MUST to indicate the names of the people you discussed a problem with; ideally you should
discuss with no more than two other people.\\
{\color{red} YOU MUST INCLUDE YOUR NAME IN THE HOMEWORK}\\
You need to submit your answer in PDF. {\LaTeX }  is typesetting is encouraged but not required.  Please write clearly and concisely - clarity and
brevity will be rewarded. Refer to known facts as necessary. 
\newline


}

\newcounter{questionno}
\setcounter{questionno}{-1}
\newcounter{partno}

\newcommand{\question}[1]{
\noindent
\newline
\stepcounter{questionno}
\setcounter{partno}{0}
{\bf Q\arabic{questionno} (#1 pts): }
}

\begin{document}

\header

\question{{\color{red}0pts correct answer,  -1,000pts incorrect answer: (0,-1,000)}} 
A correct answer to the following questions is worth 0pts. An incorrect answer is worth -1,000pts, which carries over to  other homeworks and exams, and can result in an F grade in the course.
\begin{enumerate}[(1)]
\item Student interaction with other students / individuals:
\begin{enumerate}[(a)]
\item I have copied part of my homework from another student or another person (plagiarism).
\item Yes, I discussed the homework with another person but came up with my own answers. Their name(s) is (are) \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ 
\item No, I did not discuss the homework with anyone
\end{enumerate}
\item On using online resources:
\begin{enumerate}[(a)]
\item I have copied one of my answers directly from a website (plagiarism).
\item I have used online resources to help me answer this question, but I came up with my own answers (you are allowed to use online resources as long as the answer is your own). Here is a list of the websites I have used in this homework:\\\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ 
\item I have not used any online resources except the ones provided in the course website.
\end{enumerate}


\end{enumerate}

\newpage

\noindent\textbf{Learning Objectives}: Let students understand deep learning tasks, basic feedforward neural network properties, backpropagation, and invariant representations.

\hfill

\noindent \textbf{Learning Outcomes}: After you finish this homework, you should be capable of explaining and implementing feedforward neural networks with arbitrary architectures or components from scratch.


%-----------------------------------------------

\subsection*{Concepts}

\question{2.0} 
In what follows we give a paper and ask you to classify the paper into tasks.\\ 

{\bf Mark ALL that apply and EXPLAIN YOUR ANSWERS. Answers without explanations will get deducted -0.05 points.}\\
~\\
{\bf Note 1:} This includes marking both a specific answer and its more general counterpart. E.g., Covariate shift adaptation is also a type of Domain adaptation. Your answer explanation can help assign partial credits. \\
~\\
{\bf Note 2:} {\em Papers may describe multiple tasks. Please make sure you describe which task you focused on in ``Explain Your Answers''.}. \\
~\\

{\bf Point distribution:}
\begin{itemize}
	\item Each question starts with 0.5 points.
	\item Each missing task counts as -0.1 (should be marked but was not).
	\item Each extra task counts as -0.1 (was marked but should not).
	\item Each MARKED answer not explained in ``Explain Your Answers'' gets deducted -0.05. Items left unmarked need not be explained.
		\begin{itemize}
			\item Example of an explanation: The image task in the paper is a supervised learning task: the training data is $\{(x_i,y_i)\}_i$, where $x_i$ is an image and $y_i$ is the image's label. The data $\{(x_i,y_i)\}_i$ is assumed independent, where the train and test distributions are assumed to be the same. The learning is transductive since the test data is provided during training in the form of an extra dataset where...
		\end{itemize}   
	
	\item The minimum score is zero.
\end{itemize}   

\newpage
\begin{enumerate}
\item (0.5) Kipf, Thomas N., and Max Welling.``Semi-Supervised Classification with Graph Convolutional Networks." In International Conference on Learning Representations, 2017.
\begin{enumerate}[(a)]
\item Independent observations
\item Dependent observations
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item Semi-supervised learning
\item In-distribution test data
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item Associational task (i.e., not causal)
\item Causal task
\end{enumerate}
Explain your answers\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 
\newpage
\item (0.5) Ho, Jonathan, Ajay Jain, and Pieter Abbeel. ``Denoising diffusion probabilistic models." Advances in Neural Information Processing Systems 33 (2020): 6840-6851.
\begin{enumerate}[(a)]
\item Independent observations
\item Dependent observations
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item Semi-supervised learning
\item In-distribution test data
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item Associational task (i.e., not causal)
\item Causal task
\end{enumerate}
Explain your answers\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 
 \newpage

\item (0.5) Kenton, Jacob Devlin Ming-Wei Chang, and Lee Kristina Toutanova. ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." In Proceedings of NAACL-HLT, pp. 4171-4186. 2019.
\begin{enumerate}[(a)]
\item Independent observations
\item Dependent observations
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item Semi-supervised learning
\item In-distribution test data
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item Associational task (i.e., not causal)
\item Causal task
\end{enumerate}
Explain your answers\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\
\underline{\hspace{6in}}\\

\newpage

\item (0.5) S Chandra Mouli, Bruno Ribeiro, ``Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks'', International Conference on Learning Representations, 2022. 
\begin{enumerate}[(a)]
\item Independent observations
\item Dependent observations
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item Semi-supervised learning
\item In-distribution test data
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item Associational task (i.e., not causal)
\item Causal task
\end{enumerate}
Explain your answers\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\
 \underline{\hspace{6in}}\\

\end{enumerate}

\newpage
\question{3.0} Please answer the following questions \textbf{concisely} but with enough details to get partial credit if the answer is incorrect. 

\begin{enumerate}


\item (0.5) For neural network models, regularization terms are usually applied on weight parameters. Describe why we do not regularize the bias term in the model? \\
{\bf Hint:} Biases are activation thresholds. Assume a very large regularization on the bias. Now assume an input of all zeros.
\vspace{3in}

\item (0.5)  Learning  feedforward networks with ReLUs: Could a ReLU activation cause problems when learning a model with gradient descent? Let $\{X_i\}_{i=1}^N,X_i\in \mathbb{R}^d$ be the training data. Let $W_j^{(1)}\in \mathbb{R}^d$ be the $j$-th neuron weight in the first layer. Give a non-trivial subset $\mathcal{W}$ where $\forall W_j^{(1)}\in \mathcal{W}$ the gradient $\frac{\partial L(\{X_i\}_{i=1}^N)}{\partial W_j^{(1)}}$ is zero. \\
{\bf Hint:} We say a neuron is {\em dead} when its output is constant for all training examples. \\


\newpage

\item (0.5) 
Suppose the symmetrization (Reynolds operator) $\Bar{T}$ of a finite linear transformation group $G$ has rank $0$. Prove that any $G$-invariant neuron has just the bias term.\\
{\bf Hint:} For example, a zero matrix has rank $0$.
\vspace{3in}


\item (0.5)  Consider a supervised learning task, where the training data is $(Y,X) \sim P^\text{tr}(Y,X)$ and $A \sim b$ means random variable $A$ is sampled from distribution $b$. Consider two finite linear transformation groups $G_1$ and $G_2$. Let $f_i: \mathbb{R}^d \to [0,1]$ be single neuron that is $G_i$-invariant, for $i=1,2$. Describe how we could test (and be sure) that $f_1$ is also invariant to $G_2$ or $f_2$ is also invariant to $G_1$. Assume we have access to the neuron weights ${\bf w_i}$ of $f_i$, $i=1,2$. \\
{\bf Hint 1:} Just testing the $f$'s with some transformed inputs will not guarantee they are invariant to all inputs and all transformations.\\
{\bf Hint 2:} Pay attention to the invariant subspace that defines the parameters of $f_1$ and $f_2$, which forces the neurons to be $G_1$- and $G_2$-invariant, respectively.

\newpage

\item (1.0) Consider the neurons $f_1$ and $f_2$ of the previous question. For each of the groups $i=1,2$, let $\bar{T}_i$ be the symmetrization (Reynolds) operator of group $G_i$. We also have a set of left eigenvectors vectors
${\bf v}_{i,k}^T \bar{T}_i = {\bf v}_{i,k}^T$, $k=1,\ldots,K$, for each of the groups. Explain how to create a neuron that is both $G_1$- and $G_2$-invariant. Prove that this neuron is invariant to any composition of transformations from both $G_1$ and $G_2$, such as $T_1 T_2 T_1'$, $T_1,T_1' \in G_1$, $T_2 \in G_2$.
%

\vspace{3in}



\end{enumerate}

%-----------------------------------------------

\newpage
\subsection*{Programming (5.0 pts)}

Throughout this semester you will be using Python and PyTorch as the main tool
to complete your homework, which means that getting familiar with them is required.
PyTorch (\url{http://pytorch.org/tutorials/index.html}) is a fast-growing
Deep Learning toolbox that allows you to create deep learning projects on
different levels of abstractions, from pure tensor operations to neural network
blackboxes. The official tutorial and their github
repository are your best references. Please make sure you have the latest stable version on the machine.
Linux machines with GPU installed are suggested. Moreover, following PEP8 coding style is recommended.

\hfill

\noindent \textbf{Skeleton Package}: A skeleton package is available at\\
 \url{https://www.dropbox.com/s/fsfxp1g2kgmw0kv/hw1_skeleton.zip?dl=0}. You should download it and use the folder structure provided. In some homework, skeleton code might be provided. If so, you should based on the prototype to write your implementations.

\subsection*{Introduction to PyTorch}

PyTorch, in general, provides three modules, from high-level to low-level abstractions, to build up neural networks. We are going to study 3 specific modules in this homework. First, the module that provides the highest abstraction is called \textbf{torch.nn}. It offeres layer-wise abstraction so that you can define a neural layer through a function call. For example, \textbf{torch.nn.Linear(.)} creates a fully connected layer. Coupling with contains like \textbf{Sequential(.)}, you can connect the network layer-by-layer and thus easily define your own networks. The second module is called \textbf{torch.AutoGrad}. It allows you to compute gradients with respect to all the network parameters, given the feedforwardfunction definition (the objective function). It means that you don't need to analytically compute the gradients, but only need to define the objective function while coding your networks. The last module we are going to use is \textbf{torch.tensor} which provides effecient ways of conducting tensor operations or computations so that you can customize your network in the low-level. The official PyTorch has a thorough tutorial to this (\url{http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#}). You are required to go through it and understand all three modules well before you move on.


\subsubsection*{HW Overview}

In this homework, you are going to implement vanilla feedforwardneural networks on a couple of different ways. The overall submission should be structured as below:

\hfill

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{tikzpicture}[%
  grow via three points={one child at (0.5,-0.7) and
  two children at (0.5,-0.7) and (0.5,-1.4)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \node {bruno\_ribeiro\_hw\homeworknumber}
    child { node [selected] {my\_neural\_networks}
      child { node {\_\_init\_\_.py}}
      child { node {networks.py}}
      child { node {activations.py}}
      child { node {mnist.py}}
      child { node {minibatcher.py}}
      child { node [optional] {any\_others.py}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child { node {ReadMe}}
    child { node {hw\homeworknumber\_training.py}}
    child { node {hw\homeworknumber\_demo.py}}
    child { node {hw\homeworknumber\_learning\_curves.py}}
    child { node {G-equivariant\_Auto-Encoder}
      child { node {G-equivariant-AE-skeleton.ipynb}}
      child { node {example-G-equi-ae-output.png}}
      child { node {example-ae-output.png}}
      child { node {example\_image.png}}
      };
\end{tikzpicture}

\hfill

\begin{itemize}
\item \textbf{bruno\_ribeiro\_hw\homeworknumber}: the top-level folder that contains all the files
          required in this homework. You should replace the file name with your
          name and follow the naming convention mentioned above.

\item \textbf{ReadMe}: Your ReadMe should begin with a couple of \textbf{example commands}, e.g., ``python hw\homeworknumber.py data", used to generate the outputs you report. TA would replicate your results with the commands
          provided here. More detailed options, usages and designs of your
          program can be followed. You can also list any concerns that you
          think TA should know while running your program. Note that put the
          information that you think it's more important at the top. Moreover,
          the file should be written in pure text format that can be displayed
          with Linux ``less" command.

\item \textbf{hw\homeworknumber\_training.py}: One executable we prepared for you to run
          training with your networks.

\item \textbf{hw\homeworknumber\_learning\_curves.py}: One executable for training models
          and plotting learning curves.

\item \textbf{hw\homeworknumber\_learning\_demo.py}: Demonstrate some basic Python packages. Just FYI.

\item \textbf{my\_neural\_networks}: Your Python neural network package.
          The package name in this homework is \textbf{my\_neural\_networks},
          which should NOT be changed while submitting it. Two modules should
          be at least included:
\begin{itemize}
\item \textbf{networks.py}
\item \textbf{activations.py}
\end{itemize}
Except these two modules, a package constructor \textbf{\_\_init\_\_.py} is also required for importing your modules. You are welcome to architect the package in your own favorite. For instance, adding another module, called utils.py, to facilitate your implementation.

Two additional modules, \textbf{mnist.py} and \textbf{minibatcher.py}, are also attached, and are used in the main executable to load the dataset and create minibatches (which is not needed in this homework.). You don't need to do anything with them.

\item \textbf{G-equivariant\_Auto-Encoder}: Follow the python notebook inside for constructing a G-equivariant Auto-Encoder step-by-step.

\end{itemize}


\subsubsection*{Data: MNIST}

You are going to conduct a simple classification task, called MNIST (\url{http://yann.lecun.com/exdb/mnist/}). It classifies images of hand-written digits (0-9). Each example thus is a \(28 \times 28\) image. 
\begin{itemize}
\item The full dataset contains 60k training examples and 10k testing examples.
\item We provide a data loader (read\_images(.) and read\_labels(.) in \textbf{my\_neural\_networks/mnist.py}) that will automatically download the data.
\end{itemize}


\subsubsection*{\bf Warm-up: Implement Activations}

\noindent Open the file \textbf{my\_neural\_networks/activations.py}.
As a warm up activity, you are going to implement the
\textbf{activations} module, which should realize activation functions and
objective functions that will be used in your neural networks. Note that whenever you see "raise NotImplementedError", you should implement it.

Since these
functions are mathematical equations, the code should be pretty short
and simple. The main intuition of this section is to help you get familiar
with basic Python programming, package structures, and test cases. As an
example, a Sigmoid function is already implemented in the module. Here are
the functions that you should complete:
\begin{itemize}
\item \textbf{relu}: Rectified Linear Unit (ReLU), which is defined as
  \begin{equation*}
    a_k^l = \text{relu}(z_k^l) =
    \begin{cases}
      0       & \quad \text{if } z_k^l < 0\\
      z_k^l  & \quad \text{otherwise }.
    \end{cases}
\end{equation*}

\item \textbf{softmax}: the basic softmax
  \begin{equation}
  a_k^L = \text{softmax}(z_k^L) = \frac{e^{z_k^L}}{\sum_{c} e^{z_c^L}},
  \end{equation}

\item \textbf{stable\_softmax}: the \textbf{numerically stable softmax}. You should test
if this outputs the same result as the basic softmax.
\begin{align}
\text{softmax}(x_i) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
&= \frac{C e^{x_i}}{C \sum_j e^{x_j}} \\
&= \frac{e^{x_i+\log C}}{\sum_j e^{x_j + log C}}
\end{align}
A common choice for the constant is $log C = -\max_{j} x_j$.

\item \textbf{average cross\_entropy}:
\begin{equation}
E = - \frac{1}{\text{sizeof}(\text{mini-batch})}\sum_{d \in  \text{mini-batch}} t_d \log a_k^L = - \frac{1}{\text{sizeof}(\text{mini-batch})} \sum_{d \in \text{mini-batch}} t_d (z_d^L - \log \sum_c e^{z_c^L}).
\end{equation}
where \(d\) is a data point; \(t_d\) is its true label; \(a_k^L\) is the
propability predicted by the network.

\end{itemize}

\noindent \textbf{Hints}: Make sure you tested your implementation with corner
cases before you move on. Otherwise, it would be hard to debug.

\subsubsection*{Warm-up: Understand Example Network}

\noindent Open the files \textbf{hw1\_training.py} and \textbf{my\_neural\_networks/example\_networks.py}.

\hfill


\noindent \textbf{hw1\_training.py} is the main executable (trainer). It controls in a high-level view. The task is called MNIST, which classifies images of hand-written digits. The executable uses a class called \textbf{TorchNeuralNetwork} fully implemented in \textbf{my\_neural\_networks/example\_networks.py}.


\hfill

\noindent In this task, you don't need to write any codes, but only need to play with the modules/executables provided in the skeleton and answer qeustions. A class called \textbf{TorchNeuralNetwork} is fully implemented in \textbf{my\_neural\_networks/example\_networks.py}. You can run the trainer with it by feeding correct arguments into \textbf{hw\homeworknumber\_training.py}. Read through all the related code and write down what is the correct command ("python hw\homeworknumber\_training.py" with arguments) to train such example networks in the report.

\hfill

\noindent Here is a general summary about each method in the \textbf{TorchNeuralNetwork}.
\begin{itemize}
    \item \textbf{\_\_init\_\_(self, shape, gpu\_id=-1)}: the constructor that takes
      network shape as parameters. The network weights are declared as matrices in this method.
      You should not make any changes to them, but need to think about how to use them
      to do vectorized implementations.
      \begin{itemize}
          \item Your implementation should support arbitrary network shape, rather
            than a fixed one. The shape is in specified in tuples. For exapmles,
            "shape=(784, 100, 50, 10)" means that the numbers of neurons in the
            input layer, first hidden layer, second hidden layer, and output layer
            are 784, 100, 50, and 10 respectively.
          \item All the hidden layers use \textbf{ReLU} activations.
          \item The output layer uses \textbf{Softmax} activations.
          \item \textbf{Cross-Entropy} loss should be used as the objective.
      \end{itemize}
    \item \textbf{train\_one\_epoch(self, X, y, y\_1hot, learning\_rate)}:
      conduct network training for one epoch over the given data \textbf{X}.
      It also returns the loss for the epoch.
      \begin{itemize}
          \item this method consists of three important components: feedforward, backpropagation, and weight updates.
          \item (Non-stochastic) \textbf{Gradient descent} is used. The gradient calculatation should base on all the input data. However, this part is given.
      \end{itemize}
    \item \textbf{predict(self, X)}: predicts labels for \textbf{X}.
\end{itemize}

\hfill

\noindent You need to understand the entire skeleton well at this point. \textbf{TorchNeuralNetwork} should give you a good starting point to understand all the method semantics, and the \textbf{hw\homeworknumber\_training.py} should demonstrate the training process we want. In the next task, you are going to implement another two classes supporting the same set of methods. The inputs and outputs for the methods are the same, while the internal implementations have different constrains. Therefore, make sure you understand all the method semantics and inputs/outputs before you move on.



\question{1}{\bf Implement Feedforward Neural Network with Autograd}

\noindent Open the file \textbf{my\_neural\_networks/networks.py}.

\hfill

\noindent The task here is to complete the class \textbf{AutogradNeuralNetwork}. In your implementation, several constrains are enforced:
\begin{itemize}
  \item You are NOT allowed to use any high-level neural network modules,
    such as torch.nn, unless it is specified. No credits will be given if
    similar packages or modules are used.
  \item You need to follow the methods prototypes given in the skeleton. This
    contrain might be removed in the future. However, as the first homework,
    we want you to know what do we expect you to complete in a PyTorch project.
  \item You should left at least the \textbf{hw\homeworknumber\_training.py} untouched in the final submission.
  During grading, we will replace whatever you have with the original \textbf{hw\homeworknumber\_training.py}.
\end{itemize}

\noindent For \textbf{AutogradNeuralNetwork}, you only need to complete the \textbf{feedforward part}. Other parts should already be given in the skeleton. You should be able to run the \textbf{hw\homeworknumber\_training.py} in a way similar to what you discovered in the last task. Specifically, what you need to is as follows:

\begin{itemize}
  \item Understand semantics of all the class members (variables), especially the few defined in the constructor.
  \item Identify the codes related three main components for training: feedforward, backpropagation, and weight updates.
  \item The second and third components are given. Only the \textbf{feedforward} is left for you, so go ahead and complete the \textbf{\_feed\_forward()} method.
\end{itemize}

\noindent \textbf{Things to be included in the report}:
\begin{enumerate}
  \item command line arguments for running this experiment with \textbf{hw\homeworknumber\_training.py}.
  \item Specify network shape as (784, 300, 100, 10). Collect results for \textbf{100 epochs}.
        Make two plots: "Loss vs. Epochs" and "Accuracy vs. Epochs". The
        accuracy one should include results for both training and
        testing data. Analyze and compare each plot generated in the last step. Write down your observations.
\end{enumerate}


\noindent \textbf{Hints}:
\begin{itemize}
    \item The given skeleton has all the input/output definitions. Please read through it, and if you found any typos or unclear parts, feel free to ask.
    \item In general, you don't need to change any codes given in the skeleton, unless it is for debugging.
    \item Feel free to define any helper functions/modules you need.
    \item You might need to figure out how to conduct vectorized implementations so that the pre-defined members can be utilized in a succinct and efficient way.
    \item You are welcome to use GPUs to accelerate your program
    \item For debugging, you might want to load less amount of training data to save time. This can done easily by make slight changes to \textbf{hw\homeworknumber\_training.py}.
    \item For debugging, you might want to explore some features in a Python package called \textbf{pdb}.
\end{itemize}

\newpage
\question{1}{Learning Curves: Deep vs Shallow}

\noindent Create a trainer file called \textbf{hw\homeworknumber\_learning\_curves.py}

\hfill

\noindent This executable has very similar structure to the \textbf{hw\homeworknumber\_training.py}, but you are
going to vary training data size to plot learning curves introduced in the
lecture. Specifically, you need to do the followings:
\begin{enumerate}
  \item Load MNIST data: \url{http://yann.lecun.com/exdb/mnist/} into torch
        tensors
  \item Use \textbf{AutogradNeuralNetwork}.
  \item Vary training data size in the range between 250 to 10000 in increments of 250.
  \item Train and select a model for each data size. You need to design an \textbf{early
        stop} strategy to select the model so that the learning curves will be correct.
  \item Plot learning curves for training and testing sets with
    \begin{enumerate}
      \item a network shape (784, 10)
      \item a network shape (784, 300, 100, 10)
    \end{enumerate}
\end{enumerate}

\noindent \textbf{Things that should be included in the report}:
  \begin{itemize}
    \item command line arguments for running this experiment with \textbf{hw\homeworknumber\_learning\_curves.py}.
    \item The early stop strategy you used in selecting models.
    \item The 2 learning curve plots for the 2 network shapes.
    \item Analyze and compare each plot generated in the last step. Write down
          your observations.
  \end{itemize}

\noindent \textbf{Hints}: You should understand the information embedded in
    the learning curves and what it should look like. If your implementation
    is correct, you should be able to see meaningful differences.



\newpage
\question{2.0}{Implement Backpropagation from Scratch}

\noindent Open the file \textbf{my\_neural\_networks/networks.py}.

\hfill

\noindent Implement \textbf{BasicNeuralNetwork}, but you can NOT use \textbf{torch.Autograd}. All the other instructions are similar to what is in Q2. That is, you need to implement the entire ``train\_one\_epoch'' method, including backpropagation, forward, and weight updates. For the backpropagation, you need to analytically compute the gradients.

Here, we will use pytorch the same way that we have used numpy in the lecture notes. You will need to write your own backpropagation function from scratch, following what would be the correct gradients of the already-implemented forward pass.

\hfill

\noindent \textbf{Things to be included in the report}:
\begin{enumerate}
  \item (0.5) Implement the above in the file provided (\textbf{networks.py}). Make sure your code runs with the command line arguments for running \textbf{hw\homeworknumber\_training.py}. Points will only be awarded if the code runs with the original command line.
  \item (0.5) Write down all the mathematical equations used in your backpropagation implementation.
      \vspace{5in}
  \item (0.5) Use \textbf{BasicNeuralNetwork}. Specify network shape as (784, 300, 100, 10). Collect results for \textbf{100 epochs}.
        Write in your report PDF two plots: "Loss vs. Epochs" and "Accuracy vs. Epochs". The
        accuracy one should include results for both training and
        testing data. Analyze and compare each plot generated in the last step. Write down your observations.
            \vspace{3in}
 \item (0.5) \noindent Modify \textbf{hw\homeworknumber\_learning\_curves.py} to support creating learning curves of \textbf{BasicNeuralNetwork}.
ll the other instructions are similar to what is in Q3. \textbf{Things should be included in the report}:
  \begin{itemize}
    \item Implement the above in the file provided (\textbf{hw\homeworknumber\_learning\_curves.py}). Command line arguments for running this experiment with \textbf{hw\homeworknumber\_learning\_curves.py}.
    \item The early stop strategy you used in selecting models.
    \item The 2 learning curve plots for the 2 network shapes.
    \item Analyze and compare each plot generated in the last step. Write down
          your observations in the report PDF.
  \end{itemize}

\end{enumerate}

\newpage
\question{2.0}{Implement an G-Equivariant Autoencoder}

Open the python notebook file
\textbf{G-equivariant\_Auto-Encoder/G-equivariant-AE-skeleton.ipynb}. Follow the instruction step-by-step to implement an G-equivariant Auto-Encoder that is equivariant to transformations in the group formed by 90$^o$ rotations (of bounded squared images). 

Consider a bounded square image input defined as a column vector in $\mathbb{R}^m$.
Consider a single layer whose hidden neuron output is in $\mathbb{R}^k$, such that unvectorizing it also gives a square. Let ${\bf W} \in \mathbb{R}^{k \times m}$ be the neuron parameters. G-equivariance requires that transforming the input is the same as transforming the output: $
{\bf x} \in \mathbb{R}^m, \forall g \in G , \quad \rho_2(g) {\bf W} {\bf x} =  {\bf W} \rho_1(g) {\bf x},
$ where $\rho_1:G \to \mathbb{R}^{\sqrt{m} \times \sqrt{m}}$ and $\rho_2:G \to \mathbb{R}^{\sqrt{k} \times \sqrt{k}}$. Since the above is true for all ${\bf x}$, then  $\rho_2(g) {\bf W} \rho_1(g)^{-1} =  {\bf W},$ or equivalently  $\forall g \in G, \: \rho_2(g) \otimes \rho_1(g^{-1})^T \text{vec}({\bf W})  =  \text{vec}({\bf W}),$ where vec() flattens the matrix into a vector.
Please complete the missing parts of the skeleton python notebook to implement a G-equivariant Auto-Encoder. 
    Save the .ipynb notebook as part of your tar.gz file for your code upload. 

\begin{enumerate}
    \item (0.25)  Describe all the transformations in your transformation group {\bf (in the PDF)}. For instance, $T^{90}$ the transformation that rotates the image counter-clockwise by 90 degrees must of course be present.\\
    \vspace{2in}
    \item (1.25) This part grades your  .ipynb notebook code.
     Your MLP is trained with upright digits.  Show in the .ipynb notebook that the accuracy of your outputs in both in-distribution (upright digits) and out-of-distribution (rotated digits) is approximately the same.
     Compare one predicted output of your G-equivariant MLP against the output of the traditional MLP, showing how they differ.
    \vspace{3in}
        \item (0.5)  Can you explain your choice of the dimension of the bias parameter in the G-equivariant layers. Why making a different choice could break the equivariance property?
    
\end{enumerate}
\newpage
\subsection*{Submission Instructions}

Please read the instructions carefully. Failed to follow any part might incur some score deductions.

\hfill

\subsection*{\bf PDF upload}

The report PDF must be uploaded on Gradescope (see link on Brightspace)

\hfill


\subsection*{\bf Code upload}

\noindent \textbf{Naming convention}: [firstname]\_[lastname]\_hw\homeworknumber

All your submitting code files, a ReadMe, should be included in one folder. The folder should be named with the above naming convention. For example, if my first name is "Bruno" and my last name is "Ribeiro", then for Homework \homeworknumber, my file name should be ``bruno\_ribeiro\_hw\homeworknumber''.


\hfill

\noindent \textbf{Tar your folder}: [firstname]\_[lastname]\_hw\homeworknumber.tar.gz

Remove any unnecessary files in your folder, such as training datasets. Make sure your folder structured as the tree shown in Overview section. Compress your folder with the the command: \textbf{tar czvf bruno\_ribeiro\_hw\homeworknumber.tar.gz czvf bruno\_ribeiro\_hw\homeworknumber} .

\hfill

\noindent \textbf{Submit}: 
{\bf TURNIN INSTRUCTIONS}

Please submit your compressed file on \textbf{data.cs.purdue.edu} by turnin command line, e.g.\\ \textbf{"turnin -c cs690dpl -p hw\homeworknumber{} bruno\_ribeiro\_hw\homeworknumber.tar.gz"}.
Please make sure you didn’t use any library/source explicitly forbidden to use. If such library/source code is used, you will get 0 pt for the coding part of the assignment. If your code doesn’t run on scholar.rcac.purdue.edu, then even if it compiles in another computer, your code will still be considered not-running and the respective part of the assignment will receive 0 pt.



%-----------------------------------------------



\end{document}